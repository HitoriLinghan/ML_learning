{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b73534a",
   "metadata": {},
   "source": [
    "## SetFit\n",
    "\n",
    "**SetFit**（https://huggingface.co/blog/zh/setfit） 是一个高效的少样本学习框架，它不需要提示（prompts）或标签映射器（verbalizers）就能对句子转换器（Sentence Transformers）进行微调。使用 **SetFit** 进行小样本文本分类的特点包括：\n",
    "\n",
    "1. **无需提示或标签映射器**：与其他少样本微调技术不同，SetFit 不需要手工制作的提示或标签映射器来将示例转换成适合底层语言模型的格式。它通过直接从少量标记的文本示例生成丰富的嵌入（embeddings），从而省去了这一步骤。\n",
    "\n",
    "2. **快速训练**：SetFit 不需要像T0或GPT-3这样的大型模型就能达到高准确率。因此，它在训练和推理时通常要快得多。\n",
    "\n",
    "3. **多语言支持**：SetFit 可以与 Hub 上的任何句子转换器一起使用，这意味着您可以通过简单地微调多语言检查点来对多种语言的文本进行分类。\n",
    "\n",
    "4. **样本效率高且对噪声鲁棒**：SetFit 在样本效率上显著优于标准微调，并且对噪声更加鲁棒。例如，即使每个类别只有8个标记的例子，SetFit 在客户评论（CR）情感数据集上也能与在完整训练集（3000个例子）上微调的RoBERTa Large相媲美。\n",
    "\n",
    "5. **简单性和效率**：SetFit 旨在简单高效。它首先在少量标记的示例上微调句子转换器模型（通常每类8或16个），然后在从微调的句子转换器生成的嵌入上训练分类头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "953704ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments, sample_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "322aa7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341e075df58146fdab96ccd0569127b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files='data.csv', delimiter=',',  names=[\"label\", \"review\"])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c258e08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 17\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#每个类别随机采样8条数据作为训练集\n",
    "train_dataset = sample_dataset(dataset[\"train\"], label_column=\"label\", num_samples=8)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64336a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset = dataset[\"train\"].select(range(100))\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83af798a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 900\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset[\"train\"].select(range(100, 1000))\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a437c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_body=SentenceTransformer(\"uer/sbert-base-chinese-nli\")\n",
    "#model_head=LogisticRegression(class_weight=\"balanced\") 不平衡数据集\n",
    "#model = SetFitModel(model_body=model_body,model_head=model_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24933c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name E:\\code\\model\\sbert-base-chinese-nli. Creating a new one with MEAN pooling.\n",
      "model_head.pkl not found in E:\\code\\model\\sbert-base-chinese-nli, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SetFitModel(model_body=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "), model_head=LogisticRegression(), multi_target_strategy=None, normalize_embeddings=False, labels=['正常短信', '垃圾短信'], model_card_data=SetFitModelCardData(language=None, license=None, tags=['setfit', 'sentence-transformers', 'text-classification', 'generated_from_setfit_trainer'], model_name='SetFit', model_id=None, dataset_name=None, dataset_id=None, dataset_revision=None, task_name=None, st_id=None, hyperparameters={}, eval_results_dict={}, eval_lines_list=[], metric_lines=[], widget=[], predict_example=None, label_example_list=[], tokenizer_warning=False, train_set_metrics_list=[], train_set_sentences_per_label_list=[], code_carbon_callback=None, num_classes=None, best_model_step=None, metrics=['accuracy'], pipeline_tag='text-classification', library_name='setfit', version={'python': '3.8.18', 'setfit': '1.0.3', 'sentence_transformers': '2.7.0', 'transformers': '4.40.1', 'torch': '1.12.1', 'datasets': '2.19.0', 'tokenizers': '0.19.1'}))\n"
     ]
    }
   ],
   "source": [
    "#model = SetFitModel._from_pretrained(r\"E:\\code\\model\\bge-small-en-v1.5\",)\n",
    "model = SetFitModel._from_pretrained(r\"E:\\code\\model\\sbert-base-chinese-nli\", device = torch.device(\"cuda\"))\n",
    "model.labels = [\"正常短信\", \"垃圾短信\"]\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff8c47ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to the training dataset\n",
      "Applying column mapping to the evaluation dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082934dc40fa4e9da77031591932979b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    batch_size=4,\n",
    "    num_epochs=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    metric=\"accuracy\",\n",
    "    column_mapping={\"review\": \"text\", \"label\": \"label\"}  # Map dataset columns to text/label expected by trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7424bde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 160\n",
      "  Batch size = 4\n",
      "  Num epochs = 4\n",
      "  Total optimization steps = 160\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 13:15, Epoch 4/0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Embedding Loss</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.191300</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.186300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f178c93de7e941dc8d91fbf408e686b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c379b50f948e4bc38b154c97927ff5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cc2ff9b119497d8ef3c389ac7c1a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f3d6dabde748598ff3f063dee11121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading best SentenceTransformer model from step 160.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feb78c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying column mapping to the evaluation dataset\n",
      "***** Running evaluation *****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.91}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9331fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_dataset['review'], use_labels = False)\n",
    "#print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c5be5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9959    0.9036    0.9475       809\n",
      "           1     0.5301    0.9670    0.6848        91\n",
      "\n",
      "    accuracy                         0.9100       900\n",
      "   macro avg     0.7630    0.9353    0.8162       900\n",
      "weighted avg     0.9488    0.9100    0.9209       900\n",
      "\n",
      "[[731   3]\n",
      " [ 78  88]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "classify_report = metrics.classification_report(test_dataset['label'], preds, digits = 4) #分类报告 support测试集样本数\n",
    "print(classify_report) \n",
    "confusion_matrix = metrics.confusion_matrix(preds, test_dataset['label']) #混淆矩阵\n",
    "print(confusion_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44d75cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0'], dtype='<U5')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict([\n",
    "    \"感谢致电杭州萧山全金釜韩国烧烤店，本店位于金城路xxx号。韩式烧烤等，价格实惠、欢迎惠顾【全金釜韩国烧烤店】\",\n",
    "    \"2010年12月21日10时许，被告人王某趁武强县金音建筑工地工友张某不在宿舍之际，盗窃张某枕头下现金3500元。\",\n",
    "])\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b783a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"setfit-8-shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfe2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SetFitModel.from_pretrained(\"setfit-8-shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0ba07bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0'], dtype='<U5')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict([\n",
    "    \"感谢致电杭州萧山全金釜韩国烧烤店，本店位于金城路xxx号。韩式烧烤等，价格实惠、欢迎惠顾【全金釜韩国烧烤店】\",\n",
    "    \"2010年12月21日10时许，被告人王某趁武强县金音建筑工地工友张某不在宿舍之际，盗窃张某枕头下现金3500元。\",\n",
    "])\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de54bc52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
