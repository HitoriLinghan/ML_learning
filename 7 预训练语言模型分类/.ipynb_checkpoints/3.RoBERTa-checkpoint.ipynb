{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa\n",
    "\n",
    "RoBERTa（A Robustly Optimized BERT Approach）是由Facebook AI提出的一种改进的预训练语言模型，旨在提高自然语言处理任务的性能。RoBERTa在BERT的基础上进行了一系列优化，包括使用更大的批量大小、训练更长的时间、动态掩码长度等，取得了更好的效果。\n",
    "\n",
    "RoBERTa的原理与BERT类似，都是基于Transformer架构，采用了Transformer编码器作为基础模块。RoBERTa的主要优化包括：\n",
    "\n",
    "1. **更大的批量大小**：RoBERTa使用更大的批量大小来训练模型，这有助于提高模型的训练效率和性能。\n",
    "\n",
    "2. **动态掩码长度**：RoBERTa在训练过程中使用动态掩码长度，即在每个训练步骤中随机选择掩码长度，而不是固定使用15%的掩码长度。\n",
    "\n",
    "3. **去除NSP任务**：RoBERTa去除了BERT中的Next Sentence Prediction（NSP）任务，认为这个任务并没有带来显著的性能提升。\n",
    "\n",
    "4. **更长的训练时间**：RoBERTa在训练过程中使用更长的训练时间，以获得更好的收敛效果。\n",
    "\n",
    "RoBERTa解决了一些BERT存在的问题，包括：\n",
    "\n",
    "- **数据处理不一致**：BERT在不同任务上的数据处理方式不一致，导致在某些任务上性能下降。RoBERTa通过统一数据处理方式来解决这个问题。\n",
    "\n",
    "- **掩码预测任务不合理**：BERT中的掩码预测任务（MLM）在实践中表现一般，RoBERTa去除了这个任务，并采用更合理的训练策略。\n",
    "\n",
    "RoBERTa是在2019年提出的，它在多项自然语言处理任务上取得了state-of-the-art的效果。\n",
    "\n",
    "除了RoBERTa，还有一些类似的模型，如ALBERT、XLNet、DistilBERT等，它们都是在BERT基础上进行了一定的改进和优化，取得了不错的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RoBERTa模型文件下载地址 https://huggingface.co/hfl/chinese-roberta-wwm-ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
    "from sklearn import metrics\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1243, 2)\n"
     ]
    }
   ],
   "source": [
    "#1、加载数据\n",
    "train_df = pd.read_csv('data.csv', encoding='utf-8', header=None, names=['label','review'])\n",
    "print(train_df.shape)\n",
    "\n",
    "sentences = list(train_df['review'][1:])\n",
    "label =train_df['label'][1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 token encodding\n",
    "model_path = r'E:\\code\\chinese-roberta-wwm-ext'\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "max_length=32\n",
    "sentences_tokened=tokenizer(sentences,padding=True,truncation=True,max_length=max_length, return_tensors='pt')\n",
    "label=torch.tensor(label.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[993, 249]\n"
     ]
    }
   ],
   "source": [
    "#3 encoding data\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "\n",
    "class DataToDataset(Dataset):\n",
    "    def __init__(self,encoding,labels):\n",
    "        self.encoding=encoding\n",
    "        self.labels=labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.encoding['input_ids'][index],self.encoding['attention_mask'][index],self.labels[index]\n",
    "\n",
    "#封装数据\n",
    "datasets=DataToDataset(sentences_tokened,label)\n",
    "train_size=int(len(datasets)*0.8)\n",
    "test_size=len(datasets)-train_size\n",
    "print([train_size,test_size])\n",
    "train_dataset,val_dataset=random_split(dataset=datasets,lengths=[train_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "#这里的num_workers要大于0\n",
    "train_loader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
    "val_loader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at E:\\code\\chinese-roberta-wwm-ext and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaTextClassficationModel(\n",
       "  (distilbert): RobertaForSequenceClassification(\n",
       "    (roberta): RobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768, padding_idx=0)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): RobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): RobertaClassificationHead(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4、create model\n",
    "class RobertaTextClassficationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaTextClassficationModel,self).__init__()\n",
    "        self.distilbert=RobertaForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "        \n",
    "    def forward(self,ids,mask):\n",
    "        out=self.distilbert(input_ids=ids,attention_mask=mask)\n",
    "        #print(out.shape)\n",
    "        #print(out)\n",
    "        return out[0]\n",
    "\n",
    "\n",
    "mymodel=RobertaTextClassficationModel()\n",
    "\n",
    "\n",
    "#获取gpu和cpu的设备信息\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device=\",device)\n",
    "if torch.cuda.device_count()>1:\n",
    "    print(\"Let's use \",torch.cuda.device_count(),\"GPUs!\")\n",
    "    mymodel=nn.DataParallel(mymodel)\n",
    "mymodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.roberta.embeddings.word_embeddings.weight : torch.Size([21128, 768])\n",
      "distilbert.roberta.embeddings.position_embeddings.weight : torch.Size([512, 768])\n",
      "distilbert.roberta.embeddings.token_type_embeddings.weight : torch.Size([2, 768])\n",
      "distilbert.roberta.embeddings.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.embeddings.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.0.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.0.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.0.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.0.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.0.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.0.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.0.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.1.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.1.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.1.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.1.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.1.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.1.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.1.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.2.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.2.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.2.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.2.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.2.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.2.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.2.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.3.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.3.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.3.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.3.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.3.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.3.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.3.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.4.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.4.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.4.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.4.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.4.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.4.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.4.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.5.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.5.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.5.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.5.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.5.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.5.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.5.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.6.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.6.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.6.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.6.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.6.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.6.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.6.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.7.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.7.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.7.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.7.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.7.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.7.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.7.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.8.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.8.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.8.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.8.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.8.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.8.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.8.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.9.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.9.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.9.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.9.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.9.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.9.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.9.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.10.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.10.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.10.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.10.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.10.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.10.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.10.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.11.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.11.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.11.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.11.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.11.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.11.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.11.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.classifier.dense.weight : torch.Size([768, 768])\n",
      "distilbert.classifier.dense.bias : torch.Size([768])\n",
      "distilbert.classifier.out_proj.weight : torch.Size([2, 768])\n",
      "distilbert.classifier.out_proj.bias : torch.Size([2])\n",
      "模型需要训练参数为： 102269186\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for name, parameters in mymodel.named_parameters():\n",
    "    if not parameters.requires_grad: continue\n",
    "    print(name, ':', parameters.size())\n",
    "    total_params += parameters.numel()\n",
    "print(\"模型需要训练参数为：\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1/3 epochs Batch 10 Loss:0.383828, Acc:0.825000\n",
      "train 1/3 epochs Batch 20 Loss:0.396195, Acc:0.850000\n",
      "train 1/3 epochs Batch 30 Loss:0.366358, Acc:0.870833\n",
      "train 1/3 epochs Batch 40 Loss:0.363269, Acc:0.875000\n",
      "train 1/3 epochs Batch 50 Loss:0.357927, Acc:0.877500\n",
      "train 1/3 epochs Batch 60 Loss:0.352648, Acc:0.879167\n",
      "train 1/3 epochs Loss:0.344397, Acc:0.882937\n",
      "train 2/3 epochs Batch 10 Loss:0.294893, Acc:0.881250\n",
      "train 2/3 epochs Batch 20 Loss:0.242128, Acc:0.903125\n",
      "train 2/3 epochs Batch 30 Loss:0.205164, Acc:0.918750\n",
      "train 2/3 epochs Batch 40 Loss:0.241803, Acc:0.918750\n",
      "train 2/3 epochs Batch 50 Loss:0.221557, Acc:0.926250\n",
      "train 2/3 epochs Batch 60 Loss:0.197012, Acc:0.934375\n",
      "train 2/3 epochs Loss:0.188249, Acc:0.937500\n",
      "train 3/3 epochs Batch 10 Loss:0.111006, Acc:0.962500\n",
      "train 3/3 epochs Batch 20 Loss:0.119311, Acc:0.965625\n",
      "train 3/3 epochs Batch 30 Loss:0.088671, Acc:0.975000\n",
      "train 3/3 epochs Batch 40 Loss:0.076743, Acc:0.978125\n",
      "train 3/3 epochs Batch 50 Loss:0.076226, Acc:0.978750\n",
      "train 3/3 epochs Batch 60 Loss:0.072180, Acc:0.981250\n",
      "train 3/3 epochs Loss:0.069107, Acc:0.982143\n"
     ]
    }
   ],
   "source": [
    "#5、train model\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(mymodel.parameters(),lr=0.00001)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "def flat_accuracy(preds,labels):\n",
    "    pred_flat=np.argmax(preds,axis=1).flatten()\n",
    "    labels_flat=labels.flatten()\n",
    "    return accuracy_score(labels_flat,pred_flat)\n",
    "\n",
    "epochs=3\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc=0.0\n",
    "    for i,data in enumerate(train_loader):\n",
    "        input_ids,attention_mask,labels=[elem.to(device) for elem in data]\n",
    "        #优化器置零\n",
    "        optimizer.zero_grad()\n",
    "        #得到模型的结果\n",
    "        out=mymodel(input_ids.long(),attention_mask)\n",
    "        #计算误差\n",
    "        loss=loss_func(out,labels)\n",
    "        train_loss += loss.item()\n",
    "        #误差反向传播\n",
    "        loss.backward()\n",
    "        #更新模型参数\n",
    "        optimizer.step()\n",
    "        #计算acc \n",
    "        #out=out.detach().numpy()\n",
    "        out=out.detach().cpu().numpy()\n",
    "        #labels=labels.detach().numpy()\n",
    "        labels=labels.detach().cpu().numpy()\n",
    "        train_acc+=flat_accuracy(out,labels)\n",
    "        if (i + 1) % 10 == 0:\n",
    "                print(\"train %d/%d epochs Batch %d Loss:%f, Acc:%f\" %(epoch+1,epochs, (i+1), train_loss/(i+1),train_acc/(i+1)))\n",
    "    print(\"train %d/%d epochs Loss:%f, Acc:%f\" %(epoch+1,epochs,train_loss/(i+1),train_acc/(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9913    0.9913    0.9913       229\n",
      "           1     0.9000    0.9000    0.9000        20\n",
      "\n",
      "    accuracy                         0.9839       249\n",
      "   macro avg     0.9456    0.9456    0.9456       249\n",
      "weighted avg     0.9839    0.9839    0.9839       249\n",
      "\n",
      "[[227   2]\n",
      " [  2  18]]\n"
     ]
    }
   ],
   "source": [
    "#6、evaluate\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"evaluate...\")\n",
    "pred_list = []\n",
    "y_list = []\n",
    "mymodel.eval()\n",
    "for j,batch in enumerate(val_loader):\n",
    "    val_input_ids,val_attention_mask,val_labels=[elem.to(device) for elem in batch]\n",
    "    with torch.no_grad():\n",
    "        pred=mymodel(val_input_ids,val_attention_mask)\n",
    "        pred=pred.detach().cpu().numpy()\n",
    "        pred_flat=np.argmax(pred,axis=1).flatten()\n",
    "        pred_list.extend(pred_flat)\n",
    "        val_labels=val_labels.detach().cpu().numpy()\n",
    "        y_list.extend(val_labels)\n",
    "\n",
    "classify_report = metrics.classification_report(pred_list, y_list, digits=4) #分类报告 support测试集样本数\n",
    "print(classify_report) \n",
    "confusion_matrix = metrics.confusion_matrix(pred_list, y_list) #混淆矩阵\n",
    "print(confusion_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
