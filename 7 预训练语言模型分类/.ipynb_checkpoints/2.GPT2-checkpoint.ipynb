{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2\n",
    "\n",
    "GPT-2（Generative Pre-trained Transformer 2）是OpenAI提出的一种预训练语言模型，是GPT系列的第二代模型。GPT-2基于Transformer架构，具有强大的文本生成能力，在自然语言处理领域取得了显著的成就。\n",
    "\n",
    "### GPT-2的原理如下：\n",
    "\n",
    "Transformer架构：GPT-2采用了Transformer架构，其中包括多层的自注意力机制和前馈神经网络。这种架构能够有效地捕捉输入序列中的长距离依赖关系。\n",
    "\n",
    "自回归生成：GPT-2使用自回归生成的方式，即在生成每个词时都考虑前面生成的词，通过注意力机制来捕捉上下文信息，从而生成连贯的文本。\n",
    "\n",
    "无监督预训练：GPT-2在大规模文本语料上进行无监督预训练，学习通用的语言表示。这使得模型在各种下游任务上都能够表现良好。\n",
    "\n",
    "微调：在特定的下游任务上进行微调，使得模型可以适应不同的任务需求，如文本生成、文本分类等。\n",
    "\n",
    "### GPT-2解决了传统语言模型的一些问题，包括：\n",
    "\n",
    "生成质量：GPT-2在生成文本方面取得了很高的质量，能够生成连贯、合理的文本片段。\n",
    "\n",
    "上下文理解：GPT-2通过自回归生成方式，能够充分利用上下文信息，生成更加准确的文本。\n",
    "\n",
    "多样性和创造性：GPT-2能够生成多样性的文本，有一定的创造性，可以应用于文本生成、对话系统等多个领域。\n",
    "\n",
    "GPT-2的参数规模为1.5亿个参数（small版本），最大版本的参数规模为15亿个。GPT-2在2019年发布，受到了广泛关注，并在自然语言生成领域取得了显著的进展。\n",
    "\n",
    "GPT-3和GPT-4的参数规模分别是：\n",
    "\n",
    "GPT-3：1750亿个参数。\n",
    "GPT-4：规模暂未公布，但预计会进一步增大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPT2模型文件https://huggingface.co/uer/gpt2-chinese-cluecorpussmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import BertTokenizer, GPT2ForSequenceClassification, GPT2Config\n",
    "from sklearn import metrics\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.40.1\n",
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1243, 2)\n"
     ]
    }
   ],
   "source": [
    "#1、加载数据\n",
    "train_df = pd.read_csv('data.csv', encoding='utf-8', header=None, names=['label','review'])\n",
    "print(train_df.shape)\n",
    "\n",
    "sentences = list(train_df['review'][1:])\n",
    "label =train_df['label'][1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the configuration for your model, this can be imported from transformers\n",
    "configuration = GPT2Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 token encodding\n",
    "model_path = r'E:\\code\\gpt2-chinese-cluecorpussmall'\n",
    "tokenizer=BertTokenizer.from_pretrained(model_path)\n",
    "## set up your tokenizer, just like you described, and set the pad token\n",
    "tokenizer.eos_token = tokenizer.pad_token\n",
    "max_length=32\n",
    "sentences_tokened=tokenizer(sentences,padding=True,truncation=True,max_length=max_length, return_tensors='pt')\n",
    "label=torch.tensor(label.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[993, 249]\n"
     ]
    }
   ],
   "source": [
    "#3 encoding data\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "\n",
    "class DataToDataset(Dataset):\n",
    "    def __init__(self,encoding,labels):\n",
    "        self.encoding=encoding\n",
    "        self.labels=labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.encoding['input_ids'][index],self.encoding['attention_mask'][index],self.labels[index]\n",
    "\n",
    "#封装数据\n",
    "datasets=DataToDataset(sentences_tokened,label)\n",
    "train_size=int(len(datasets)*0.8)\n",
    "test_size=len(datasets)-train_size\n",
    "print([train_size,test_size])\n",
    "train_dataset,val_dataset=random_split(dataset=datasets,lengths=[train_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "#这里的num_workers要大于0\n",
    "train_loader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
    "val_loader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at E:\\code\\gpt2-chinese-cluecorpussmall and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2TextClassficationModel(\n",
       "  (distilbert): GPT2ForSequenceClassification(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(21128, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): GPT2Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPT2Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): GPT2MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (score): Linear(in_features=768, out_features=2, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4、create model\n",
    "class GPT2TextClassficationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GPT2TextClassficationModel,self).__init__()\n",
    "        self.distilbert=GPT2ForSequenceClassification(configuration).from_pretrained(model_path, num_labels=2)\n",
    "        # set the pad token of the model's configuration\n",
    "        self.distilbert.config.pad_token_id = self.distilbert.config.eos_token_id\n",
    "        \n",
    "    def forward(self,ids,mask):\n",
    "        out=self.distilbert(input_ids=ids,attention_mask=mask)\n",
    "        #print(out.shape)\n",
    "        #print(out)\n",
    "        return out[0]\n",
    "\n",
    "\n",
    "mymodel=GPT2TextClassficationModel()\n",
    "\n",
    "#获取gpu和cpu的设备信息\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device=\",device)\n",
    "if torch.cuda.device_count()>1:\n",
    "    print(\"Let's use \",torch.cuda.device_count(),\"GPUs!\")\n",
    "    mymodel=nn.DataParallel(mymodel)\n",
    "mymodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1/3 epochs Batch 10 Loss:0.385499, Acc:0.781250\n",
      "train 1/3 epochs Batch 20 Loss:0.316752, Acc:0.846875\n",
      "train 1/3 epochs Batch 30 Loss:0.236989, Acc:0.891667\n",
      "train 1/3 epochs Batch 40 Loss:0.219348, Acc:0.901563\n",
      "train 1/3 epochs Batch 50 Loss:0.190997, Acc:0.917500\n",
      "train 1/3 epochs Batch 60 Loss:0.172897, Acc:0.923958\n",
      "train 1/3 epochs Loss:0.166283, Acc:0.927579\n",
      "train 2/3 epochs Batch 10 Loss:0.069036, Acc:0.987500\n",
      "train 2/3 epochs Batch 20 Loss:0.063258, Acc:0.990625\n",
      "train 2/3 epochs Batch 30 Loss:0.047014, Acc:0.993750\n",
      "train 2/3 epochs Batch 40 Loss:0.044587, Acc:0.993750\n",
      "train 2/3 epochs Batch 50 Loss:0.050186, Acc:0.991250\n",
      "train 2/3 epochs Batch 60 Loss:0.048678, Acc:0.991667\n",
      "train 2/3 epochs Loss:0.047353, Acc:0.992063\n",
      "train 3/3 epochs Batch 10 Loss:0.014103, Acc:0.993750\n",
      "train 3/3 epochs Batch 20 Loss:0.019296, Acc:0.990625\n",
      "train 3/3 epochs Batch 30 Loss:0.014419, Acc:0.993750\n",
      "train 3/3 epochs Batch 40 Loss:0.022364, Acc:0.992188\n",
      "train 3/3 epochs Batch 50 Loss:0.025056, Acc:0.991250\n",
      "train 3/3 epochs Batch 60 Loss:0.025085, Acc:0.991667\n",
      "train 3/3 epochs Loss:0.024503, Acc:0.992063\n"
     ]
    }
   ],
   "source": [
    "#5、train model\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(mymodel.parameters(),lr=0.00001)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "def flat_accuracy(preds,labels):\n",
    "    pred_flat=np.argmax(preds,axis=1).flatten()\n",
    "    labels_flat=labels.flatten()\n",
    "    return accuracy_score(labels_flat,pred_flat)\n",
    "\n",
    "epochs=3\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc=0.0\n",
    "    for i,data in enumerate(train_loader):\n",
    "        input_ids,attention_mask,labels=[elem.to(device) for elem in data]\n",
    "        #优化器置零\n",
    "        optimizer.zero_grad()\n",
    "        #得到模型的结果\n",
    "        out=mymodel(input_ids.long(),attention_mask)\n",
    "        #计算误差\n",
    "        loss=loss_func(out,labels)\n",
    "        train_loss += loss.item()\n",
    "        #误差反向传播\n",
    "        loss.backward()\n",
    "        #更新模型参数\n",
    "        optimizer.step()\n",
    "        #计算acc \n",
    "        #out=out.detach().numpy()\n",
    "        out=out.detach().cpu().numpy()\n",
    "        #labels=labels.detach().numpy()\n",
    "        labels=labels.detach().cpu().numpy()\n",
    "        train_acc+=flat_accuracy(out,labels)\n",
    "        if (i + 1) % 10 == 0:\n",
    "                print(\"train %d/%d epochs Batch %d Loss:%f, Acc:%f\" %(epoch+1,epochs, (i+1), train_loss/(i+1),train_acc/(i+1)))\n",
    "    print(\"train %d/%d epochs Loss:%f, Acc:%f\" %(epoch+1,epochs,train_loss/(i+1),train_acc/(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9909    0.9732    0.9820       224\n",
      "           1     0.7931    0.9200    0.8519        25\n",
      "\n",
      "    accuracy                         0.9679       249\n",
      "   macro avg     0.8920    0.9466    0.9169       249\n",
      "weighted avg     0.9710    0.9679    0.9689       249\n",
      "\n",
      "[[218   6]\n",
      " [  2  23]]\n"
     ]
    }
   ],
   "source": [
    "#6、evaluate\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"evaluate...\")\n",
    "pred_list = []\n",
    "y_list = []\n",
    "mymodel.eval()\n",
    "for j,batch in enumerate(val_loader):\n",
    "    val_input_ids,val_attention_mask,val_labels=[elem.to(device) for elem in batch]\n",
    "    with torch.no_grad():\n",
    "        pred=mymodel(val_input_ids,val_attention_mask)\n",
    "        pred=pred.detach().cpu().numpy()\n",
    "        pred_flat=np.argmax(pred,axis=1).flatten()\n",
    "        pred_list.extend(pred_flat)\n",
    "        val_labels=val_labels.detach().cpu().numpy()\n",
    "        y_list.extend(val_labels)\n",
    "\n",
    "classify_report = metrics.classification_report(pred_list, y_list, digits=4) #分类报告 support测试集样本数\n",
    "print(classify_report) \n",
    "confusion_matrix = metrics.confusion_matrix(pred_list, y_list) #混淆矩阵\n",
    "print(confusion_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
