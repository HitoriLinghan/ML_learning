{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa\n",
    "\n",
    "RoBERTa（A Robustly Optimized BERT Approach）是由Facebook AI提出的一种改进的预训练语言模型，旨在提高自然语言处理任务的性能。RoBERTa在BERT的基础上进行了一系列优化，包括使用更大的批量大小、训练更长的时间、动态掩码长度等，取得了更好的效果。\n",
    "\n",
    "RoBERTa的原理与BERT类似，都是基于Transformer架构，采用了Transformer编码器作为基础模块。RoBERTa的主要优化包括：\n",
    "\n",
    "1. **更大的批量大小**：RoBERTa使用更大的批量大小来训练模型，这有助于提高模型的训练效率和性能。\n",
    "\n",
    "2. **动态掩码长度**：RoBERTa在训练过程中使用动态掩码长度，即在每个训练步骤中随机选择掩码长度，而不是固定使用15%的掩码长度。\n",
    "\n",
    "3. **去除NSP任务**：RoBERTa去除了BERT中的Next Sentence Prediction（NSP）任务，认为这个任务并没有带来显著的性能提升。\n",
    "\n",
    "4. **更长的训练时间**：RoBERTa在训练过程中使用更长的训练时间，以获得更好的收敛效果。\n",
    "\n",
    "RoBERTa解决了一些BERT存在的问题，包括：\n",
    "\n",
    "- **数据处理不一致**：BERT在不同任务上的数据处理方式不一致，导致在某些任务上性能下降。RoBERTa通过统一数据处理方式来解决这个问题。\n",
    "\n",
    "- **掩码预测任务不合理**：BERT中的掩码预测任务（MLM）在实践中表现一般，RoBERTa去除了这个任务，并采用更合理的训练策略。\n",
    "\n",
    "RoBERTa是在2019年提出的，它在多项自然语言处理任务上取得了state-of-the-art的效果。\n",
    "\n",
    "除了RoBERTa，还有一些类似的模型，如ALBERT、XLNet、DistilBERT等，它们都是在BERT基础上进行了一定的改进和优化，取得了不错的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RoBERTa模型文件下载地址 https://huggingface.co/hfl/chinese-roberta-wwm-ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from transformers import AutoTokenizer, RobertaForSequenceClassification\n",
    "from sklearn import metrics\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1243, 2)\n"
     ]
    }
   ],
   "source": [
    "#1、加载数据\n",
    "train_df = pd.read_csv('data.csv', encoding='utf-8', header=None, names=['label','review'])\n",
    "print(train_df.shape)\n",
    "\n",
    "sentences = list(train_df['review'][1:])\n",
    "label =train_df['label'][1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\download\\sd\\sd-webui-aki-v4.5\\python\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#2 token encodding\n",
    "model_path = r'E:\\code\\chinese-roberta-wwm-ext'\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "max_length=32\n",
    "sentences_tokened=tokenizer(sentences,padding=True,truncation=True,max_length=max_length, return_tensors='pt')\n",
    "label=torch.tensor(label.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[993, 249]\n"
     ]
    }
   ],
   "source": [
    "#3 encoding data\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "\n",
    "class DataToDataset(Dataset):\n",
    "    def __init__(self,encoding,labels):\n",
    "        self.encoding=encoding\n",
    "        self.labels=labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.encoding['input_ids'][index],self.encoding['attention_mask'][index],self.labels[index]\n",
    "\n",
    "#封装数据\n",
    "datasets=DataToDataset(sentences_tokened,label)\n",
    "train_size=int(len(datasets)*0.8)\n",
    "test_size=len(datasets)-train_size\n",
    "print([train_size,test_size])\n",
    "train_dataset,val_dataset=random_split(dataset=datasets,lengths=[train_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "#这里的num_workers要大于0\n",
    "train_loader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)\n",
    "val_loader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,shuffle=True,num_workers=0)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load the configuration of 'E:\\code\\chinese-roberta-wwm-ext'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'E:\\code\\chinese-roberta-wwm-ext' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mf:\\download\\sd\\sd-webui-aki-v4.5\\python\\lib\\site-packages\\transformers\\configuration_utils.py:629\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 629\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    643\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n",
      "File \u001b[1;32mf:\\download\\sd\\sd-webui-aki-v4.5\\python\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mf:\\download\\sd\\sd-webui-aki-v4.5\\python\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\download\\sd\\sd-webui-aki-v4.5\\python\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'E:\\code\\chinese-roberta-wwm-ext'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m#print(out.shape)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m#print(out)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 14\u001b[0m mymodel\u001b[38;5;241m=\u001b[39m\u001b[43mRobertaTextClassficationModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#获取gpu和cpu的设备信息\u001b[39;00m\n\u001b[0;32m     18\u001b[0m device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m, in \u001b[0;36mRobertaTextClassficationModel.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m(RobertaTextClassficationModel,\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert\u001b[38;5;241m=\u001b[39m\u001b[43mRobertaForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\download\\sd\\sd-webui-aki-v4.5\\python\\lib\\site-packages\\transformers\\modeling_utils.py:2305\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m   2304\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[1;32m-> 2305\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m   2306\u001b[0m         config_path,\n\u001b[0;32m   2307\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2308\u001b[0m         return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2309\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   2310\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m   2311\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   2312\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2313\u001b[0m         use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[0;32m   2314\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2315\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m   2316\u001b[0m         _from_auto\u001b[38;5;241m=\u001b[39mfrom_auto_class,\n\u001b[0;32m   2317\u001b[0m         _from_pipeline\u001b[38;5;241m=\u001b[39mfrom_pipeline,\n\u001b[0;32m   2318\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2319\u001b[0m     )\n\u001b[0;32m   2320\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2321\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32mf:\\download\\sd\\sd-webui-aki-v4.5\\python\\lib\\site-packages\\transformers\\configuration_utils.py:547\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrainedConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    471\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m--> 547\u001b[0m     config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[0;32m    549\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    550\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    551\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    552\u001b[0m         )\n",
      "File \u001b[1;32mf:\\download\\sd\\sd-webui-aki-v4.5\\python\\lib\\site-packages\\transformers\\configuration_utils.py:574\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    572\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    573\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 574\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    576\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mf:\\download\\sd\\sd-webui-aki-v4.5\\python\\lib\\site-packages\\transformers\\configuration_utils.py:650\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    649\u001b[0m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[1;32m--> 650\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    651\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load the configuration of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    653\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name. Otherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    654\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfiguration_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    655\u001b[0m         )\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;66;03m# Load config dict\u001b[39;00m\n\u001b[0;32m    659\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load the configuration of 'E:\\code\\chinese-roberta-wwm-ext'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'E:\\code\\chinese-roberta-wwm-ext' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "#4、create model\n",
    "class RobertaTextClassficationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaTextClassficationModel,self).__init__()\n",
    "        self.distilbert=RobertaForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "        \n",
    "    def forward(self,ids,mask):\n",
    "        out=self.distilbert(input_ids=ids,attention_mask=mask)\n",
    "        #print(out.shape)\n",
    "        #print(out)\n",
    "        return out[0]\n",
    "\n",
    "\n",
    "mymodel=RobertaTextClassficationModel()\n",
    "\n",
    "\n",
    "#获取gpu和cpu的设备信息\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device=\",device)\n",
    "if torch.cuda.device_count()>1:\n",
    "    print(\"Let's use \",torch.cuda.device_count(),\"GPUs!\")\n",
    "    mymodel=nn.DataParallel(mymodel)\n",
    "mymodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.roberta.embeddings.word_embeddings.weight : torch.Size([21128, 768])\n",
      "distilbert.roberta.embeddings.position_embeddings.weight : torch.Size([512, 768])\n",
      "distilbert.roberta.embeddings.token_type_embeddings.weight : torch.Size([2, 768])\n",
      "distilbert.roberta.embeddings.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.embeddings.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.0.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.0.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.0.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.0.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.0.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.0.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.0.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.0.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.1.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.1.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.1.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.1.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.1.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.1.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.1.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.1.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.2.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.2.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.2.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.2.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.2.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.2.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.2.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.2.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.3.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.3.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.3.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.3.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.3.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.3.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.3.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.3.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.4.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.4.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.4.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.4.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.4.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.4.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.4.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.4.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.5.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.5.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.5.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.5.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.5.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.5.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.5.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.5.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.6.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.6.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.6.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.6.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.6.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.6.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.6.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.6.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.7.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.7.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.7.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.7.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.7.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.7.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.7.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.7.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.8.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.8.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.8.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.8.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.8.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.8.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.8.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.8.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.9.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.9.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.9.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.9.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.9.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.9.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.9.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.9.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.10.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.10.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.10.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.10.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.10.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.10.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.10.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.10.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.attention.self.query.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.11.attention.self.query.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.attention.self.key.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.11.attention.self.key.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.attention.self.value.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.11.attention.self.value.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.attention.output.dense.weight : torch.Size([768, 768])\n",
      "distilbert.roberta.encoder.layer.11.attention.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.attention.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.attention.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.intermediate.dense.weight : torch.Size([3072, 768])\n",
      "distilbert.roberta.encoder.layer.11.intermediate.dense.bias : torch.Size([3072])\n",
      "distilbert.roberta.encoder.layer.11.output.dense.weight : torch.Size([768, 3072])\n",
      "distilbert.roberta.encoder.layer.11.output.dense.bias : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.output.LayerNorm.weight : torch.Size([768])\n",
      "distilbert.roberta.encoder.layer.11.output.LayerNorm.bias : torch.Size([768])\n",
      "distilbert.classifier.dense.weight : torch.Size([768, 768])\n",
      "distilbert.classifier.dense.bias : torch.Size([768])\n",
      "distilbert.classifier.out_proj.weight : torch.Size([2, 768])\n",
      "distilbert.classifier.out_proj.bias : torch.Size([2])\n",
      "模型需要训练参数为： 102269186\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for name, parameters in mymodel.named_parameters():\n",
    "    if not parameters.requires_grad: continue\n",
    "    print(name, ':', parameters.size())\n",
    "    total_params += parameters.numel()\n",
    "print(\"模型需要训练参数为：\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 1/3 epochs Batch 10 Loss:0.383828, Acc:0.825000\n",
      "train 1/3 epochs Batch 20 Loss:0.396195, Acc:0.850000\n",
      "train 1/3 epochs Batch 30 Loss:0.366358, Acc:0.870833\n",
      "train 1/3 epochs Batch 40 Loss:0.363269, Acc:0.875000\n",
      "train 1/3 epochs Batch 50 Loss:0.357927, Acc:0.877500\n",
      "train 1/3 epochs Batch 60 Loss:0.352648, Acc:0.879167\n",
      "train 1/3 epochs Loss:0.344397, Acc:0.882937\n",
      "train 2/3 epochs Batch 10 Loss:0.294893, Acc:0.881250\n",
      "train 2/3 epochs Batch 20 Loss:0.242128, Acc:0.903125\n",
      "train 2/3 epochs Batch 30 Loss:0.205164, Acc:0.918750\n",
      "train 2/3 epochs Batch 40 Loss:0.241803, Acc:0.918750\n",
      "train 2/3 epochs Batch 50 Loss:0.221557, Acc:0.926250\n",
      "train 2/3 epochs Batch 60 Loss:0.197012, Acc:0.934375\n",
      "train 2/3 epochs Loss:0.188249, Acc:0.937500\n",
      "train 3/3 epochs Batch 10 Loss:0.111006, Acc:0.962500\n",
      "train 3/3 epochs Batch 20 Loss:0.119311, Acc:0.965625\n",
      "train 3/3 epochs Batch 30 Loss:0.088671, Acc:0.975000\n",
      "train 3/3 epochs Batch 40 Loss:0.076743, Acc:0.978125\n",
      "train 3/3 epochs Batch 50 Loss:0.076226, Acc:0.978750\n",
      "train 3/3 epochs Batch 60 Loss:0.072180, Acc:0.981250\n",
      "train 3/3 epochs Loss:0.069107, Acc:0.982143\n"
     ]
    }
   ],
   "source": [
    "#5、train model\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(mymodel.parameters(),lr=0.00001)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "def flat_accuracy(preds,labels):\n",
    "    pred_flat=np.argmax(preds,axis=1).flatten()\n",
    "    labels_flat=labels.flatten()\n",
    "    return accuracy_score(labels_flat,pred_flat)\n",
    "\n",
    "epochs=3\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc=0.0\n",
    "    for i,data in enumerate(train_loader):\n",
    "        input_ids,attention_mask,labels=[elem.to(device) for elem in data]\n",
    "        #优化器置零\n",
    "        optimizer.zero_grad()\n",
    "        #得到模型的结果\n",
    "        out=mymodel(input_ids.long(),attention_mask)\n",
    "        #计算误差\n",
    "        loss=loss_func(out,labels)\n",
    "        train_loss += loss.item()\n",
    "        #误差反向传播\n",
    "        loss.backward()\n",
    "        #更新模型参数\n",
    "        optimizer.step()\n",
    "        #计算acc \n",
    "        #out=out.detach().numpy()\n",
    "        out=out.detach().cpu().numpy()\n",
    "        #labels=labels.detach().numpy()\n",
    "        labels=labels.detach().cpu().numpy()\n",
    "        train_acc+=flat_accuracy(out,labels)\n",
    "        if (i + 1) % 10 == 0:\n",
    "                print(\"train %d/%d epochs Batch %d Loss:%f, Acc:%f\" %(epoch+1,epochs, (i+1), train_loss/(i+1),train_acc/(i+1)))\n",
    "    print(\"train %d/%d epochs Loss:%f, Acc:%f\" %(epoch+1,epochs,train_loss/(i+1),train_acc/(i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9913    0.9913    0.9913       229\n",
      "           1     0.9000    0.9000    0.9000        20\n",
      "\n",
      "    accuracy                         0.9839       249\n",
      "   macro avg     0.9456    0.9456    0.9456       249\n",
      "weighted avg     0.9839    0.9839    0.9839       249\n",
      "\n",
      "[[227   2]\n",
      " [  2  18]]\n"
     ]
    }
   ],
   "source": [
    "#6、evaluate\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"evaluate...\")\n",
    "pred_list = []\n",
    "y_list = []\n",
    "mymodel.eval()\n",
    "for j,batch in enumerate(val_loader):\n",
    "    val_input_ids,val_attention_mask,val_labels=[elem.to(device) for elem in batch]\n",
    "    with torch.no_grad():\n",
    "        pred=mymodel(val_input_ids,val_attention_mask)\n",
    "        pred=pred.detach().cpu().numpy()\n",
    "        pred_flat=np.argmax(pred,axis=1).flatten()\n",
    "        pred_list.extend(pred_flat)\n",
    "        val_labels=val_labels.detach().cpu().numpy()\n",
    "        y_list.extend(val_labels)\n",
    "\n",
    "classify_report = metrics.classification_report(pred_list, y_list, digits=4) #分类报告 support测试集样本数\n",
    "print(classify_report) \n",
    "confusion_matrix = metrics.confusion_matrix(pred_list, y_list) #混淆矩阵\n",
    "print(confusion_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
