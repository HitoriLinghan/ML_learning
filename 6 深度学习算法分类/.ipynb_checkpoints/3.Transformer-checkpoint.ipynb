{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "Transformer模型是一种基于自注意力机制（Self-Attention）的深度学习模型，它在2017年由Google的研究者在论文《Attention Is All You Need》中首次提出¹²³。\n",
    "\n",
    "**特点**：\n",
    "- **自注意力机制**：Transformer使用自注意力机制来处理序列数据，这使得每个元素都能直接与序列中的其他元素交互和获取信息。\n",
    "- **并行计算**：与传统的循环神经网络（RNN）相比，Transformer能够更好地利用现代硬件进行并行计算，从而加快训练速度。\n",
    "- **无需递归**：Transformer完全摒弃了递归结构，这减少了模型的复杂性并提高了效率。\n",
    "- **多头注意力**：通过多头注意力机制，模型能够同时关注输入序列的不同部分，捕捉丰富的上下文信息。\n",
    "\n",
    "**解决的问题**：\n",
    "- **长距离依赖问题**：在处理长序列数据时，RNN和LSTM等传统模型容易受到梯度消失或爆炸的影响，难以捕捉长距离依赖关系。Transformer通过自注意力机制有效地解决了这一问题。\n",
    "- **并行化难题**：RNN由于其递归特性，难以实现有效的并行化。Transformer的结构使得模型可以充分利用现代计算资源进行并行处理。\n",
    "\n",
    "**来源**：\n",
    "- Transformer模型首次出现在2017年的论文《Attention Is All You Need》中。\n",
    "\n",
    "Transformer模型的提出标志着自然语言处理领域的一个重要转折点，它的设计理念和架构已经成为了后续许多模型的基础，如BERT、GPT等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import numpy as np;\n",
    "from gensim.models import KeyedVectors\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  文本  标签\n",
      "0                       商业秘密 秘密性 维系 商业价值 垄断 地位 前提条件    0\n",
      "1  南口 阿玛施 新春 第一批 限量 春装 店 春暖花开 淑女 裙冰 蓝色 公主 衫 气质 粉小...   1\n",
      "2                                 带给 常州 一场 壮观 视觉 盛宴    0\n",
      "3                                     原因 不明 泌尿系统 结石    0\n",
      "4                                    年 盐城 拉回来 麻麻 嫁妆    0\n"
     ]
    }
   ],
   "source": [
    "#读取数据\n",
    "df = pd.read_csv('分词后data.csv')\n",
    "df = df.dropna()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1241 1241\n",
      "Counter({0: 1119, 1: 122})\n"
     ]
    }
   ],
   "source": [
    "data = df['文本'].tolist()\n",
    "label = df['标签'].tolist()\n",
    "print(len(data), len(label)) #查看语料信息\n",
    "print(Counter(label)) #查看不同标签文本数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['商业秘密 秘密性 维系 商业价值 垄断 地位 前提条件 ', '南口 阿玛施 新春 第一批 限量 春装 店 春暖花开 淑女 裙冰 蓝色 公主 衫 气质 粉小 西装 冰丝 女王 长半裙 皇 ', '带给 常州 一场 壮观 视觉 盛宴 ', '原因 不明 泌尿系统 结石 ', '年 盐城 拉回来 麻麻 嫁妆 ']\n"
     ]
    }
   ],
   "source": [
    "texts = [each.split() for each in data]\n",
    "print(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建词表，将文本中的字符单词替换为数字索引\n",
    "word_vocb=[]\n",
    "word_vocb.append('')\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        word_vocb.append(word)\n",
    "word_vocb=set(word_vocb)\n",
    "vocb_size=len(word_vocb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5919\n"
     ]
    }
   ],
   "source": [
    "print(vocb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词表与索引的映射\n",
    "word_to_idx={word:i for i,word in enumerate(word_vocb)}\n",
    "idx_to_word={word_to_idx[word]:word for word in word_to_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282\n",
      "噪音\n"
     ]
    }
   ],
   "source": [
    "print(word_to_idx['商业价值'])\n",
    "print(idx_to_word[222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#演示文本最大长度设置为30\n",
    "max_len = 30\n",
    "#生成训练数据，删除超过max_len的部分，不够的补0\n",
    "texts_with_id=np.zeros([len(texts),max_len])\n",
    "for i in range(0,len(texts)):\n",
    "    if len(texts[i])<max_len:\n",
    "        for j in range(0,len(texts[i])):\n",
    "            texts_with_id[i][j]=word_to_idx[texts[i][j]]\n",
    "        for j in range(len(texts[i]),max_len):\n",
    "            texts_with_id[i][j] = word_to_idx['']\n",
    "    else:\n",
    "        for j in range(0,max_len):\n",
    "            texts_with_id[i][j]=word_to_idx[texts[i][j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1241, 30)\n",
      "[4751. 5814. 4514.  282. 2059. 1049. 5804.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.    0.    0.    0.]\n"
     ]
    }
   ],
   "source": [
    "print(texts_with_id.shape)\n",
    "print(texts_with_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer模型\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(Transformer, self).__init__()\n",
    "        vocb_size = args['vocb_size']\n",
    "        dim = args['dim'] #词向量维度\n",
    "        n_class = args['n_class']\n",
    "        pad_size = args['max_len'] # 每句话处理成的长度(短填长切)\n",
    "        embedding_matrix=args['embedding_matrix']\n",
    "        hidden_size = 128 #隐藏层单元\n",
    "        num_layers = 2 #RNN层数\n",
    "        dropout = 0.5 #防过拟合随机丢失\n",
    "        #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n",
    "        device = 'cpu'\n",
    "        dim_model = 300\n",
    "        hidden = 1024\n",
    "        last_hidden = 512\n",
    "        num_head = 5\n",
    "        num_encoder = 2\n",
    "        \n",
    "        #需要将事先训练好的词向量载入\n",
    "        self.embedding = nn.Embedding(vocb_size, dim,_weight=embedding_matrix)\n",
    "        \n",
    "        self.postion_embedding = Positional_Encoding(dim, pad_size, dropout, device)\n",
    "        \n",
    "        self.encoder = Encoder(dim_model, num_head, hidden, dropout)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            copy.deepcopy(self.encoder)\n",
    "            # Encoder(config.dim_model, config.num_head, config.hidden, config.dropout)\n",
    "            for _ in range(num_encoder)])\n",
    "\n",
    "        self.fc1 = nn.Linear(pad_size * dim_model, num_layers)\n",
    "        # self.fc2 = nn.Linear(last_hidden, num_layers)\n",
    "        # self.fc1 = nn.Linear(dim_model, num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.postion_embedding(out)\n",
    "        for encoder in self.encoders:\n",
    "            out = encoder(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        # out = torch.mean(out, 1)\n",
    "        out = self.fc1(out)\n",
    "        return out\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_model, num_head, hidden, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attention = Multi_Head_Attention(dim_model, num_head, dropout)\n",
    "        self.feed_forward = Position_wise_Feed_Forward(dim_model, hidden, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.attention(x)\n",
    "        out = self.feed_forward(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self, embed, pad_size, dropout, device):\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.device = device\n",
    "        self.pe = torch.tensor([[pos / (10000.0 ** (i // 2 * 2.0 / embed)) for i in range(embed)] for pos in range(pad_size)])\n",
    "        self.pe[:, 0::2] = np.sin(self.pe[:, 0::2])\n",
    "        self.pe[:, 1::2] = np.cos(self.pe[:, 1::2])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + nn.Parameter(self.pe, requires_grad=False).to(self.device)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Scaled_Dot_Product_Attention(nn.Module):\n",
    "    '''Scaled Dot-Product Attention '''\n",
    "    def __init__(self):\n",
    "        super(Scaled_Dot_Product_Attention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, scale=None):\n",
    "        '''\n",
    "        Args:\n",
    "            Q: [batch_size, len_Q, dim_Q]\n",
    "            K: [batch_size, len_K, dim_K]\n",
    "            V: [batch_size, len_V, dim_V]\n",
    "            scale: 缩放因子 论文为根号dim_K\n",
    "        Return:\n",
    "            self-attention后的张量，以及attention张量\n",
    "        '''\n",
    "        attention = torch.matmul(Q, K.permute(0, 2, 1))\n",
    "        if scale:\n",
    "            attention = attention * scale\n",
    "        # if mask:  # TODO change this\n",
    "        #     attention = attention.masked_fill_(mask == 0, -1e9)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        context = torch.matmul(attention, V)\n",
    "        return context\n",
    "\n",
    "\n",
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, dim_model, num_head, dropout=0.0):\n",
    "        super(Multi_Head_Attention, self).__init__()\n",
    "        self.num_head = num_head\n",
    "        assert dim_model % num_head == 0\n",
    "        self.dim_head = dim_model // self.num_head\n",
    "        self.fc_Q = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.fc_K = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.fc_V = nn.Linear(dim_model, num_head * self.dim_head)\n",
    "        self.attention = Scaled_Dot_Product_Attention()\n",
    "        self.fc = nn.Linear(num_head * self.dim_head, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        Q = self.fc_Q(x)\n",
    "        K = self.fc_K(x)\n",
    "        V = self.fc_V(x)\n",
    "        Q = Q.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        K = K.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        V = V.view(batch_size * self.num_head, -1, self.dim_head)\n",
    "        # if mask:  # TODO\n",
    "        #     mask = mask.repeat(self.num_head, 1, 1)  # TODO change this\n",
    "        scale = K.size(-1) ** -0.5  # 缩放因子\n",
    "        context = self.attention(Q, K, V, scale)\n",
    "\n",
    "        context = context.view(batch_size, -1, self.dim_head * self.num_head)\n",
    "        out = self.fc(context)\n",
    "        out = self.dropout(out)\n",
    "        out = out + x  # 残差连接\n",
    "        out = self.layer_norm(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Position_wise_Feed_Forward(nn.Module):\n",
    "    def __init__(self, dim_model, hidden, dropout=0.0):\n",
    "        super(Position_wise_Feed_Forward, self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_model, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, dim_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + x  # 残差连接\n",
    "        out = self.layer_norm(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGPT 生成的代码解释\n",
    "\n",
    "这段代码定义了一个Transformer模型，用于文本分类任务。下面是对代码的解释：\n",
    "\n",
    "1. `class Transformer(nn.Module):`：定义了一个名为Transformer的PyTorch模型类，继承自`nn.Module`。\n",
    "\n",
    "2. `def __init__(self,args):`：模型的初始化方法，接收一个参数`args`，其中包含了模型所需的各种参数。\n",
    "\n",
    "3. 在`__init__`方法中定义了许多参数，包括词汇表大小、词向量维度、类别数等。同时还定义了一些Transformer模型的超参数，如隐藏层单元数量、RNN层数、防止过拟合的随机丢失率等。\n",
    "\n",
    "4. `self.embedding = nn.Embedding(vocb_size, dim,_weight=embedding_matrix)`：定义一个词嵌入层，用于将词的索引转换为词向量。这里使用了事先训练好的词向量矩阵作为初始权重。\n",
    "\n",
    "5. `self.postion_embedding = Positional_Encoding(dim, pad_size, dropout, device)`：定义一个位置编码层，用于加入位置信息到词向量中。\n",
    "\n",
    "6. `self.encoder = Encoder(dim_model, num_head, hidden, dropout)`：定义了一个编码器，包含多层Transformer的编码器。\n",
    "\n",
    "7. `def forward(self, x):`：定义模型的前向传播方法，接收输入x，返回模型的输出。\n",
    "\n",
    "8. `out = self.embedding(x)`：将输入x通过词嵌入层转换为词向量。\n",
    "\n",
    "9. `out = self.postion_embedding(out)`：将词向量加上位置编码。\n",
    "\n",
    "10. `for encoder in self.encoders:`：遍历多层编码器。\n",
    "\n",
    "11. `out = encoder(out)`：将输入通过编码器得到输出。\n",
    "\n",
    "12. `out = out.view(out.size(0), -1)`：将输出展平为一维向量。\n",
    "\n",
    "13. `out = self.fc1(out)`：使用全连接层将输出映射到类别空间。\n",
    "\n",
    "整个模型包含了Embedding层、Positional Encoding层、多层Encoder层和全连接层。通过这些组件，Transformer模型可以有效地捕捉输入序列中的信息，并输出用于分类的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "word_dim = 300 #词向量的维度\n",
    "n_class = 2 #类别\n",
    "\n",
    "#textCNN调用的参数\n",
    "args['vocb_size']=vocb_size\n",
    "args['max_len']=max_len\n",
    "args['n_class']=n_class\n",
    "args['dim']=word_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec词向量\n",
    "cn_model = KeyedVectors.load_word2vec_format('F:/sgns.weibo.word.bz2', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding层的参数大小为vocb_size*dim，即词汇表大小乘词向量的维度 又称为lookup表\n",
    "embedding_matrix = np.zeros((vocb_size, word_dim))\n",
    "\n",
    "for word, i in word_to_idx.items():\n",
    "    if word in cn_model:\n",
    "        embedding_vector = cn_model[word]\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "args['embedding_matrix']=torch.Tensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embedding): Embedding(5919, 300)\n",
      "  (postion_embedding): Positional_Encoding(\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (attention): Multi_Head_Attention(\n",
      "      (fc_Q): Linear(in_features=300, out_features=300, bias=True)\n",
      "      (fc_K): Linear(in_features=300, out_features=300, bias=True)\n",
      "      (fc_V): Linear(in_features=300, out_features=300, bias=True)\n",
      "      (attention): Scaled_Dot_Product_Attention()\n",
      "      (fc): Linear(in_features=300, out_features=300, bias=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (feed_forward): Position_wise_Feed_Forward(\n",
      "      (fc1): Linear(in_features=300, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=300, bias=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (encoders): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (attention): Multi_Head_Attention(\n",
      "        (fc_Q): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (fc_K): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (fc_V): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (attention): Scaled_Dot_Product_Attention()\n",
      "        (fc): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (feed_forward): Position_wise_Feed_Forward(\n",
      "        (fc1): Linear(in_features=300, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=300, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (attention): Multi_Head_Attention(\n",
      "        (fc_Q): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (fc_K): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (fc_V): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (attention): Scaled_Dot_Product_Attention()\n",
      "        (fc): Linear(in_features=300, out_features=300, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (feed_forward): Position_wise_Feed_Forward(\n",
      "        (fc1): Linear(in_features=300, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=300, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (layer_norm): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=9000, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#构建Transformer模型\n",
    "rnn=Transformer(args)\n",
    "print(rnn) #输出模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight : torch.Size([5919, 300])\n",
      "encoder.attention.fc_Q.weight : torch.Size([300, 300])\n",
      "encoder.attention.fc_Q.bias : torch.Size([300])\n",
      "encoder.attention.fc_K.weight : torch.Size([300, 300])\n",
      "encoder.attention.fc_K.bias : torch.Size([300])\n",
      "encoder.attention.fc_V.weight : torch.Size([300, 300])\n",
      "encoder.attention.fc_V.bias : torch.Size([300])\n",
      "encoder.attention.fc.weight : torch.Size([300, 300])\n",
      "encoder.attention.fc.bias : torch.Size([300])\n",
      "encoder.attention.layer_norm.weight : torch.Size([300])\n",
      "encoder.attention.layer_norm.bias : torch.Size([300])\n",
      "encoder.feed_forward.fc1.weight : torch.Size([1024, 300])\n",
      "encoder.feed_forward.fc1.bias : torch.Size([1024])\n",
      "encoder.feed_forward.fc2.weight : torch.Size([300, 1024])\n",
      "encoder.feed_forward.fc2.bias : torch.Size([300])\n",
      "encoder.feed_forward.layer_norm.weight : torch.Size([300])\n",
      "encoder.feed_forward.layer_norm.bias : torch.Size([300])\n",
      "encoders.0.attention.fc_Q.weight : torch.Size([300, 300])\n",
      "encoders.0.attention.fc_Q.bias : torch.Size([300])\n",
      "encoders.0.attention.fc_K.weight : torch.Size([300, 300])\n",
      "encoders.0.attention.fc_K.bias : torch.Size([300])\n",
      "encoders.0.attention.fc_V.weight : torch.Size([300, 300])\n",
      "encoders.0.attention.fc_V.bias : torch.Size([300])\n",
      "encoders.0.attention.fc.weight : torch.Size([300, 300])\n",
      "encoders.0.attention.fc.bias : torch.Size([300])\n",
      "encoders.0.attention.layer_norm.weight : torch.Size([300])\n",
      "encoders.0.attention.layer_norm.bias : torch.Size([300])\n",
      "encoders.0.feed_forward.fc1.weight : torch.Size([1024, 300])\n",
      "encoders.0.feed_forward.fc1.bias : torch.Size([1024])\n",
      "encoders.0.feed_forward.fc2.weight : torch.Size([300, 1024])\n",
      "encoders.0.feed_forward.fc2.bias : torch.Size([300])\n",
      "encoders.0.feed_forward.layer_norm.weight : torch.Size([300])\n",
      "encoders.0.feed_forward.layer_norm.bias : torch.Size([300])\n",
      "encoders.1.attention.fc_Q.weight : torch.Size([300, 300])\n",
      "encoders.1.attention.fc_Q.bias : torch.Size([300])\n",
      "encoders.1.attention.fc_K.weight : torch.Size([300, 300])\n",
      "encoders.1.attention.fc_K.bias : torch.Size([300])\n",
      "encoders.1.attention.fc_V.weight : torch.Size([300, 300])\n",
      "encoders.1.attention.fc_V.bias : torch.Size([300])\n",
      "encoders.1.attention.fc.weight : torch.Size([300, 300])\n",
      "encoders.1.attention.fc.bias : torch.Size([300])\n",
      "encoders.1.attention.layer_norm.weight : torch.Size([300])\n",
      "encoders.1.attention.layer_norm.bias : torch.Size([300])\n",
      "encoders.1.feed_forward.fc1.weight : torch.Size([1024, 300])\n",
      "encoders.1.feed_forward.fc1.bias : torch.Size([1024])\n",
      "encoders.1.feed_forward.fc2.weight : torch.Size([300, 1024])\n",
      "encoders.1.feed_forward.fc2.bias : torch.Size([300])\n",
      "encoders.1.feed_forward.layer_norm.weight : torch.Size([300])\n",
      "encoders.1.feed_forward.layer_norm.bias : torch.Size([300])\n",
      "fc1.weight : torch.Size([2, 9000])\n",
      "fc1.bias : torch.Size([2])\n",
      "模型需要训练参数为： 4728074\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for name, parameters in rnn.named_parameters():\n",
    "    if not parameters.requires_grad: continue\n",
    "    print(name, ':', parameters.size())\n",
    "    total_params += parameters.numel()\n",
    "print(\"模型需要训练参数为：\", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1241\n"
     ]
    }
   ],
   "source": [
    "#参数设置\n",
    "\n",
    "EPOCH = 5; #轮次，根据训练情况设置\n",
    "\n",
    "LR = 0.001 #学习率，根据训练情况设置\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR) #优化器\n",
    "#损失函数\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "#训练批次大小，和内存显存相关\n",
    "epoch_size=100;\n",
    "texts_len=len(texts_with_id)\n",
    "print(texts_len)\n",
    "#划分训练数据和测试数据\n",
    "x_train, x_test, y_train, y_test = train_test_split(texts_with_id, label, test_size=0.2, random_state=42)\n",
    " \n",
    "test_x=torch.LongTensor(x_test)\n",
    "test_y=torch.LongTensor(y_test)\n",
    "train_x=x_train\n",
    "train_y=y_train\n",
    " \n",
    "test_epoch_size=200;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992, 30)\n",
      "[0, 1, 0, 0, 0, 0, 1, 0, 1, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(label[0:10])\n",
    "print(y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0 损失:tensor(0.6320)\n",
      "batch: 1 损失:tensor(2.8240)\n",
      "batch: 2 损失:tensor(1.3480)\n",
      "batch: 3 损失:tensor(0.2346)\n",
      "batch: 4 损失:tensor(5.2503)\n",
      "batch: 5 损失:tensor(0.3456)\n",
      "batch: 6 损失:tensor(0.7860)\n",
      "batch: 7 损失:tensor(1.2058)\n",
      "batch: 8 损失:tensor(1.6717)\n",
      "epoch 0 训练集准确率：0.7016129032258065 测试集准确率：0.7188755020080321\n",
      "batch: 0 损失:tensor(1.0603)\n",
      "batch: 1 损失:tensor(2.5234)\n",
      "batch: 2 损失:tensor(1.2582)\n",
      "batch: 3 损失:tensor(0.4836)\n",
      "batch: 4 损失:tensor(0.2890)\n",
      "batch: 5 损失:tensor(1.8217)\n",
      "batch: 6 损失:tensor(0.2649)\n",
      "batch: 7 损失:tensor(0.5454)\n",
      "batch: 8 损失:tensor(0.9145)\n",
      "epoch 1 训练集准确率：0.7358870967741935 测试集准确率：0.7188755020080321\n",
      "batch: 0 损失:tensor(0.6578)\n",
      "batch: 1 损失:tensor(1.8156)\n",
      "batch: 2 损失:tensor(1.0508)\n",
      "batch: 3 损失:tensor(0.6016)\n",
      "batch: 4 损失:tensor(0.5647)\n",
      "batch: 5 损失:tensor(0.4683)\n",
      "batch: 6 损失:tensor(0.2923)\n",
      "batch: 7 损失:tensor(0.3579)\n",
      "batch: 8 损失:tensor(0.5645)\n",
      "epoch 2 训练集准确率：0.8024193548387096 测试集准确率：0.7068273092369478\n",
      "batch: 0 损失:tensor(0.3119)\n",
      "batch: 1 损失:tensor(0.5432)\n",
      "batch: 2 损失:tensor(0.4272)\n",
      "batch: 3 损失:tensor(0.2913)\n",
      "batch: 4 损失:tensor(0.3187)\n",
      "batch: 5 损失:tensor(0.3362)\n",
      "batch: 6 损失:tensor(0.2717)\n",
      "batch: 7 损失:tensor(0.3081)\n",
      "batch: 8 损失:tensor(0.3346)\n",
      "epoch 3 训练集准确率：0.8185483870967742 测试集准确率：0.7269076305220884\n",
      "batch: 0 损失:tensor(0.3067)\n",
      "batch: 1 损失:tensor(0.4380)\n",
      "batch: 2 损失:tensor(0.3179)\n",
      "batch: 3 损失:tensor(0.2558)\n",
      "batch: 4 损失:tensor(0.2323)\n",
      "batch: 5 损失:tensor(0.3018)\n",
      "batch: 6 损失:tensor(0.2706)\n",
      "batch: 7 损失:tensor(0.2975)\n",
      "batch: 8 损失:tensor(0.2960)\n",
      "epoch 4 训练集准确率：0.8175403225806451 测试集准确率：0.7188755020080321\n",
      "batch: 0 损失:tensor(0.2629)\n",
      "batch: 1 损失:tensor(0.3862)\n",
      "batch: 2 损失:tensor(0.3541)\n",
      "batch: 3 损失:tensor(0.2723)\n",
      "batch: 4 损失:tensor(0.2178)\n",
      "batch: 5 损失:tensor(0.2796)\n",
      "batch: 6 损失:tensor(0.2384)\n",
      "batch: 7 损失:tensor(0.2880)\n",
      "batch: 8 损失:tensor(0.3010)\n",
      "epoch 5 训练集准确率：0.8185483870967742 测试集准确率：0.7188755020080321\n",
      "batch: 0 损失:tensor(0.1986)\n",
      "batch: 1 损失:tensor(0.3332)\n",
      "batch: 2 损失:tensor(0.3197)\n",
      "batch: 3 损失:tensor(0.1936)\n",
      "batch: 4 损失:tensor(0.1816)\n",
      "batch: 5 损失:tensor(0.2118)\n",
      "batch: 6 损失:tensor(0.1681)\n",
      "batch: 7 损失:tensor(0.2009)\n",
      "batch: 8 损失:tensor(0.1482)\n",
      "epoch 6 训练集准确率：0.8336693548387096 测试集准确率：0.751004016064257\n",
      "batch: 0 损失:tensor(0.2451)\n",
      "batch: 1 损失:tensor(0.1059)\n",
      "batch: 2 损失:tensor(0.1328)\n",
      "batch: 3 损失:tensor(0.1428)\n",
      "batch: 4 损失:tensor(0.1234)\n",
      "batch: 5 损失:tensor(0.1055)\n",
      "batch: 6 损失:tensor(0.0547)\n",
      "batch: 7 损失:tensor(0.1465)\n",
      "batch: 8 损失:tensor(0.1534)\n",
      "epoch 7 训练集准确率：0.8699596774193549 测试集准确率：0.7751004016064257\n",
      "batch: 0 损失:tensor(0.1363)\n",
      "batch: 1 损失:tensor(0.1219)\n",
      "batch: 2 损失:tensor(0.1551)\n",
      "batch: 3 损失:tensor(0.1254)\n",
      "batch: 4 损失:tensor(0.0989)\n",
      "batch: 5 损失:tensor(0.0886)\n",
      "batch: 6 损失:tensor(0.0485)\n",
      "batch: 7 损失:tensor(0.0789)\n",
      "batch: 8 损失:tensor(0.1376)\n",
      "epoch 8 训练集准确率：0.8790322580645161 测试集准确率：0.7670682730923695\n",
      "batch: 0 损失:tensor(0.0925)\n",
      "batch: 1 损失:tensor(0.1095)\n",
      "batch: 2 损失:tensor(0.1130)\n",
      "batch: 3 损失:tensor(0.1288)\n",
      "batch: 4 损失:tensor(0.0954)\n",
      "batch: 5 损失:tensor(0.0679)\n",
      "batch: 6 损失:tensor(0.0318)\n",
      "batch: 7 损失:tensor(0.1284)\n",
      "batch: 8 损失:tensor(0.1119)\n",
      "epoch 9 训练集准确率：0.8840725806451613 测试集准确率：0.7831325301204819\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    train_acc_all = 0\n",
    "    for i in range(0, math.ceil(len(train_x)/epoch_size)):\n",
    " \n",
    "        b_x = Variable(torch.LongTensor(train_x[i*epoch_size:i*epoch_size+epoch_size]))\n",
    " \n",
    "        b_y = Variable(torch.LongTensor((train_y[i*epoch_size:i*epoch_size+epoch_size])))\n",
    "        output = rnn(b_x)\n",
    "        loss = loss_function(output, b_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('batch: ' + str(i) + \" 损失:\" + str(loss.data))\n",
    "        pred_y = torch.max(output, 1)[1].data.squeeze()\n",
    "        acc = (b_y == pred_y)\n",
    "        acc = acc.numpy().sum()\n",
    "        train_acc_all = train_acc_all + acc\n",
    " \n",
    "    acc_all = 0;\n",
    "    for j in range(0, math.ceil(len(test_x) / test_epoch_size)):\n",
    "        b_x = Variable(torch.LongTensor(test_x[j * test_epoch_size:j * test_epoch_size + test_epoch_size]))\n",
    "        b_y = Variable(torch.LongTensor((test_y[j * test_epoch_size:j * test_epoch_size + test_epoch_size])))\n",
    "        test_output = rnn(b_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "        # print(pred_y)\n",
    "        # print(test_y)\n",
    "        acc = (pred_y == b_y)\n",
    "        acc = acc.numpy().sum()\n",
    "        #print(\"准确率 \" + str(acc / b_y.size(0)))\n",
    "        acc_all = acc_all + acc\n",
    " \n",
    "    train_accuracy = train_acc_all / len(train_y)\n",
    "    test_accuracy = acc_all / (test_y.size(0))\n",
    "    print(\"epoch \" + str(epoch) + \" \" + \"训练集准确率：\" + str(train_accuracy) + \" 测试集准确率：\" + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        正常短信     0.9819    0.9775    0.9797       222\n",
      "        垃圾短信     0.8214    0.8519    0.8364        27\n",
      "\n",
      "    accuracy                         0.9639       249\n",
      "   macro avg     0.9017    0.9147    0.9080       249\n",
      "weighted avg     0.9645    0.9639    0.9641       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_output = rnn(test_x)\n",
    "pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "#输出结果报告\n",
    "print(classification_report(test_y, pred_y, digits=4, target_names = ['正常短信', '垃圾短信']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "torchgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
